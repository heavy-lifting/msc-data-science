{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM2KUrDSjLjH2jplVxcBN4z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uxxJWftNjFX6"},"outputs":[],"source":["# use MLPCLassifier from sklearn.neural_network\n","\n","# see slides for detail"]},{"cell_type":"code","source":["# cnn in tensorflow\n","!pip install -U --pre tensorflow==\"2.*\"\n","!pip install tf_slim\n","\n","import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D\n","\n","Conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=“same”, activation=“relu”)\n","\n","# strides = how many pixels we move across while convoluting - 1 gives us the same size response as input, but we can make this larger to downsample\n","# padding imputes pixels round the edge to ensure response is same size as input"],"metadata":{"id":"l8U2G9k-0T6v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define model with layers\n","class MyModel(Model):\n","  def __init__(self):\n","  super(MyModel, self).__init__()\n","  self.conv1 = Conv2D(32, 3, activation=\"relu\") #define a convolutional layer\n","  self.pool1 = MaxPooling2D(pool_size=2) #define a max-pooling layer\n","  self.flatten = Flatten() #convert from 2D matrix to vector\n","  self.d1 = Dense(128, activation=\"relu\") #define fully connected layer\n","  self.d2 = Dense(10) #define another fully connected layer\n","\n","def call(self, x):\n","  x = self.conv1(x)\n","  x = self.pool1(x)\n","  x = self.flatten(x)\n","  x = self.d1(x)\n","  return self.d2(x)\n","  #forward pipeline to take input"],"metadata":{"id":"Pnp_qr3A0YWg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train a model\n","model = MyModel()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam()\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","\n","def train_step(images, labels):\n","  with tf.GradientTape() as tape:\n","    predictions = model(images, training=True)\n","    loss = loss_object(labels, predictions) # how close predictions are to real labels\n","  gradients = tape.gradient(loss, model.trainable_variables) # get the gradients to reduce loss (I think)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # update weights\n","\n","for images, labels in train_ds:\n","  train_step(images, labels)"],"metadata":{"id":"LzSP-Vtn0dNt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# above is how to build it from scratch - but we can alos just use standard architecture from keras\n","\n","# use default functions to train a model\n","model = keras.applications.resnet50.ResNet50(weights =\"imagenet\") # use weights based on image net - use of pre-trained model rather than random initial weights\n","model. compile (loss=\"sparse categorical crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n","history=model.fit (X_train, y_train, epochs=10, validation data=(X_valid, y_valid))\n","score = model.evaluate (X_test, y_test)\n","X_new = X_test[:10] # pretend we have new images\n","y_pred=model.predict (X_new)"],"metadata":{"id":"rTDQ9oqL0hE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data on goodle drive\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","#!ls 'drive/MyDrive/imagenet_sub'\n","train_dir2 = 'drive/MyDrive/imagenet_sub/train'\n","train_dataset2 = image_dataset_from_directory(train_dir2,shuffle=Tr\n","ue,batch_size=BATCH_SIZE,image_size=IMG_SIZE)"],"metadata":{"id":"5VHkcodk0lpA"},"execution_count":null,"outputs":[]}]}